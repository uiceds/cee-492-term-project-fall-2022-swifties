## Predictive Model {.page_break_before}


Road crash prediction models are very useful tools in highway safety, given their potential for determining both the crash frequency occurrence and the degree severity of crashes (Abdulhafedh, 2017). While crash frequency refers to the number of predicted crashes for a given road under specific conditions, crash severity aims to correlate the casualties with contributing factors such as driver behavior, road conditions, and external factors (weather, lightning, etc). Identifying and analyzing the attributes influencing forecasting accuracy is of great importance in road crash prediction (Rashidi et al, 2022). 

In a road crash dataset, the fatal crash samples often constitute a very small proportion in comparison with non-fatal crash samples. That is also the case for the dataset obtaned fro this work, even after the whole process of cleaning. The accurate prediction of fatal crashes, as a minority class, is one of the important challenges in such imbalanced sample distribution in most machine learning algorithms (Danesh et al, 2017). On top of that, several other factors such as the traffic flow or the average speed can greatly influence the prediction, so assumptions have to be made in order to develop a prediction model. Given the nature of this database, the prediction model to be developed will focus on estimating crash severity based on our known attributes. For the reasons established before, crash frequency predictions would require traffic data. Thus, trying to estimate it without this specific independent variable would lead to a completely innacurate model. In the previous section, the Exploratory Data Analysis provided insightful information regarding the correlation of the independent variables, and a regression model will be the first approach for crash severity prediction. In this section, attention will be placed on understanding the contribution of every independent variable to the overall result, to later start working on solving the data imbalance issue already identified.

The steps to be carried out can be summarized as follows:

1. Assign numerical values to the classification attributes to further study the influence of each factor in our target value.
2. Analyze the correlation between independent variables, and filter out those who are highly correlated.
3. Define an error metric and build a regression model. 
4. Train the model to find the parameters that minimize the error metric.
5. Divide the database into training data and test data.
6. Compare the predicted crash severity with both training data and test data, and assess the preliminary results.


### Model Description

Given the nature of the database and the primary established goal of predicting the severity of crashes according to different combinations of scenarios, a classification problem is faced. The first approach will be the development of a Decision Tree (DT) scheme. Decision Trees are non-parametric supervised learning methods, that can deal with large datasets without imposing complicated parametric structures, enabling them to predict the value of a target variable based on simple decision rules inferred from the data features. The objective is to find a set of decision rules that naturally partition the feature space to provide an informative and robust hierarchical classification model (Myles et al, 2004). 

The dataset in this study is large enough (with hundreds of thousands of entries), so advantage could be taken from this by dividing it into training and validation datasets. Another option derives from the fact that the IDOT's crash databases from different years are also freely available online on .csv format. Therefore, additional hundreds of thousands of entries could be used for validation (e.g. the data from the years immediatly before the ones selected for building the dataset of this work, such as the 2016 dataset). This way, the full dataset will be used to train and build a DT model, and the validation dataset will afterwards be useful to to decide on the appropriate tree size needed to achieve the optimal model accuracy.

### One Hot Encoding

Machine learning algorithms are generally unable to work with categorical data when fed directly into the model. Therefore, there is a need to convert our independent variables (inputs) :RoadSurfaceCond,:RoadDefects,:LightingCond and :WeatherCond into numbers, into numbers and the same will be required for our output variable since it will also be categorical (:CrashSeverity).

The task of assigning numerical values to make use of them has to be handled with the aim of avoiding undesired biases in the assignment process. If we assigned a float or a integer value, our machine learning model may wrongly allocate a higher weight to variables with higher values, affecting the accuracy of the prediction model.

To avoid this issue, we will encode our categorical features as one-hot numeric arrays. The one-hot encoding scheme, also known as ‘one-of-K’ or ‘dummy’ creates a binary colum for each category, and returns a sparse matrix or dense array (depending on the sparse parameters).Our inputs to this transform will be strings, denoting the values taken on by our categorical (discrete) features and our output will be a binary feature for each possible category with the value of 1 to the feature of each sample that belongs to the category, and a value of 0 for any other feature (Buitinck et al, 2013). An example is shown below for the variable *:LightingCond*:

| Original Feature |  One-Hot Encoded Feature | 
|:-----------------|:------------------------:|
| Darkness         |    [1, 0, 0, 0, 0, 0]    |  
| Darkness/Lighted |    [0, 1, 0, 0, 0, 0]    | 
| Dawn             |    [0, 0, 1, 0, 0, 0]    | 
| Daylight         |    [0, 0, 0, 1, 0, 0]    | 
| Dusk             |    [0, 0, 0, 0, 1, 0]    | 
| Unknown          |    [0, 0, 0, 0, 0, 1]    | 

Although, the One Hot Encoding technique will be useful to transform and preprocess our data so that our model can understand it better and learn from it more effectively, it comes with its own advatanges and disadvantages. We'll be analizing the results as we test our model to report our findings. 

