# Predictive Model {.page_break_before}

Given the nature of the database and the primary established goal of predicting the severity of crashes according to different combinations of scenarios, a classification problem is faced, for which the following models will be tested:

- a) Decision Tree
- b) Random Forest
- c) Convolutional Neural Network (CNN) 

The dataset in this study is large enough (with hundreds of thousands of entries), so an advantage could be taken from this by dividing it into training and validation datasets. Another option derives from the fact that the IDOT's crash databases for different years are also freely available online in .csv format. Therefore, additional hundreds of thousands of entries could be used for validation (e.g. the data from the years immediately before the ones selected for building the dataset of this report, such as the 2016 dataset). This way, the full dataset could be used to train and build the model, and the validation datasets will afterwards be useful to decide on the appropriate model parameters needed to achieve the optimal model accuracy. 

Since machine learning algorithms are generally unable to work with categorical data when fed directly into the model, there is a need to convert our independent variables (inputs):RoadSurfaceCond,:RoadDefects,:LightingCond and :WeatherCond into numbers, and the same will be required for our output variable since it will also be categorical (:CrashSeverity). The task of assigning numerical values to make use of them has to be handled with the aim of avoiding undesired biases in the assignment process. If we assigned a float or a integer value, our machine learning model may wrongly allocate a higher weight to variables with higher values, affecting the accuracy of the prediction model. To avoid this issue, we will encode our categorical features as one-hot numeric arrays, a technique that is presented hereafter.

### One Hot Encoding

The one-hot encoding scheme, also known as ‘one-of-K’ or ‘dummy’ creates a binary colum for each category, and returns a sparse matrix or dense array (depending on the sparse parameters).Our inputs to this transform will be strings, denoting the values taken on by our categorical (discrete) features and our output will be a binary feature for each possible category with the value of 1 to the feature of each sample that belongs to the category, and a value of 0 for any other feature (Buitinck et al, 2013). An example is shown below for the variable *:LightingCond*:

| Original Feature |  One-Hot Encoded Feature | 
|:-----------------|:------------------------:|
| Darkness         |    [1, 0, 0, 0, 0, 0]    |  
| Darkness/Lighted |    [0, 1, 0, 0, 0, 0]    | 
| Dawn             |    [0, 0, 1, 0, 0, 0]    | 
| Daylight         |    [0, 0, 0, 1, 0, 0]    | 
| Dusk             |    [0, 0, 0, 0, 1, 0]    | 
| Unknown          |    [0, 0, 0, 0, 0, 1]    | 

Although, the One Hot Encoding technique will be useful to transform and preprocess our data so that our model can understand it better and learn from it more effectively, it comes with its own advantages and disadvantages. We'll be analyzing the results as we test our model to report our findings. 

Once the data has been transformed, we can use it to train our proposed models.
The description, adequacy, results, and conclusions from every model tested is discussed in the following sections.


## Models

### a) Decision Tree Model

The first approach will be the development of a Decision Tree (DT) scheme. Decision Trees are non-parametric supervised learning methods, that can deal with large datasets without imposing complicated parametric structures, enabling them to predict the value of a target variable based on simple decision rules inferred from the data features. The objective is to find a set of decision rules that naturally partition the feature space to provide an informative and robust hierarchical classification model (Myles et al, 2004). 

The DecisionTree.jl package available for Julia, provided us with a "DecisionTreeClassifier" model. This model requires the following hyperparameters:

-	pruning_purity_threshold: (post-pruning) merge leaves having >=thresh combined purity (default: no pruning)
-	max_depth: maximum depth of the decision tree (default: no maximum)
-	min_samples_leaf: the minimum number of samples each leaf needs to have (default: 1)
-	min_samples_split: the minimum number of samples in needed for a split (default: 2)
-	min_purity_increase: minimum purity needed for a split (default: 0.0)
-	n_subfeatures: number of features to select at random (default: keep all)
-	rng: the random number generator to use. Can be an Int, which will be used to seed and create a new random number generator.

As the first approach, the values by default have been kept constant while the maximum depth of the decision tree is altered from scenario to scenario to evaluate the changes in the result. 
The first proposed model (Decision Tree) resulted in limited accuracy (78%) on the training data using the fully cleaned combined dataset. As a first estimate, this value could be seen as promising, however several issues could be immediately noticed. Despite testing several model parameters, the accuracy of the model did not improve considerably what indicates a problem in our dataset. Let’s look at the general structure of our decision tree to understand this.

![
**Decision Tree Structure.**
Using package: DecisionTreeClassifier.jl.
]( https://user-images.githubusercontent.com/63623246/202955035-4e2c3ee6-60ab-4d00-a1e8-95dd54139032.png "DT_Structure"){#fig:DT_Structure}

Figure 10 below shows the confusion plot for the decision tree model. It can be seen from the plot that model is predicting 691050 accurate values for "Property Damage" and 111 non-accurate values as "Injury" instead of "Property Damage". The prediction accuracy is much worse for the other two labels. For  "Injury", the model predicted only 185 accurate values and predicted 189752 values for "Property Damage" instead of "Injury". Similarily, for "Fatal", the model predicted only 2 accurate values and inaccurately predicted 2692 "Property Damage" instead of "Fatal". This shows that the model is predicting "Property Damage" for most of the cases.


![
**Confusion plot for the Decision Tree model**
](https://user-images.githubusercontent.com/112972950/204965595-4ffaac2b-ff2b-4450-9829-ae2e5e6e7b58.png "ConDT"){#fig: ConDT}

This issue can be summarized by stating that one of the possible outputs (“Property Damage”), the lowest severity crash type, dominates the dataset, accounting for almost 78% of the total number of cases. Consequently, a simple model that only predicts “Property Damage”, independently of the entries, would have 78% accuracy. This imposes a huge bias in the criteria being used by the model to predict an output. As seen in the previous figure, most of the leaves of the decision tree get the right output just because the likelihood of predicting one of them is enormously higher than the likelihood of any of the other two.

Alternative strategies were brainstormed to overcome the imbalanced data issue. First, the DT models were re-trained using a database comprised of a sample of equal numbers of observations for the 3 different classes in the expected output vector. As this test didn’t bring a considerable change either, a Random Forest (RF) model was also run for both datasets (full and reduced, equally sampled). 

### b) Random Forest

The Random Forest (RF) models are used to predict both categorical and continuous outputs. The background of the RF concept recalls for the presence of multiple classification trees, which partition the data using a sequence of binary splits on individual variables. The non-split nodes are called terminal nodes.  Given the presence of multiple Decision Trees (DTs), the RF models use the bagging method to build decision trees as parallel estimators, which are finally averaged to give rise to the mean predictive model. It should be noted that improved RF estimations can be obtained by taking into account uncorrelated and difference between DTs, otherwise the final accuracy of the RF and DT models would be similar. In Julia, the so-called “RandomForestClassifier” object can be used to build a RF model. 

Given the limitations of the obtained DT results, a Random Forest (RF) model was implemented in order to evaluate if any increase in the model accuracy could be obtained. However, the accuracy obtained by the Random Forest model was also 78%. Figure 11 below shows the confusion plot for the random forest model.
It can be seen in the figure that the model is working well only for "Property Damage" where it is predicting 691119 accurate values and only 42 non accurate ones. For "Injury", the model is predicting 117 accurate values and is wrongly prediciting 189821 "Property Damage" values instead. In addition, for "Fatal", the model is only predicting one accurate value and for the remaining ones, it is predicting "Property Damage" instead. Comapring the decison tree and the random forest models, the random forest is working even better for "Property Damage" and worse for the other 2, meaning, it is predicting more accurate values as "Property Damage" and less accurate values for "Injury" and "Fatal" keeping the overall accuracy the same at 78%.

![
**Confusion plot for the Random Forest model**
](https://user-images.githubusercontent.com/112972950/204968729-0acd38b3-028d-4289-a41c-e06183024271.png "ConRF"){#fig: ConRF}


### c) Different Approaches for the Decision Tree and Random Forest Models
After achieving an accuracy of 78% for the  decision tree and random forest models that were tested, different approaches were implemented to modify and adjust the data with the goal of achieving a higher accuracy in the predictions. These approaches are described below:

#### 1-	Removing rows with entries that were not very well understood
As mentioned earlier, the columns “RoadSurfaceCond”, “RoadDefects”, “LightingCond” and “WeatherCond” were the independent variables for this model. Each of these columns (attributes) has different possible values. Table 3 below shows the unique values for each attribute. For example, and as can be seen in the table, WeatherCond has 12 different possible values and for the one-hot encoding, this would create 12 columns just for the attribute “WeatherCond”.  It was thought that reducing the number of the one-hot encoded columns would make the accuracy better as the model would not have as many columns to use for the prediction of the models.
Thus, it was decided to delete those rows including the label "Other" or "Unknown" in the attributes. These two values were chosen to be removed because they do not offer any significant or specific information about the corresponding attributes.

| RoadSurfaceCond      |  RoadDefects             |  LightingCond            |  WeatherCond             |
|:---------------------|:------------------------:|:------------------------:|:------------------------:|
| Wet                  |  Debris on Roadway       | Darkness                 |  Blowing Sand            |
| Dry                  |  No Defects              | Darkness/Lighted Road    |  Blowing Snow            |
| Ice                  |  Other                   | Dawn                     |  Clear                   |
| Snow or Slush        |  Rut Holes               | Daylight                 |  Cloudy/Overcast         |
| Unknown              |  Shoulders               | Dusk                     |  Fog/Smoke/Haze          |
| Other                |  Unknown                 |                          |  Freezing Rain           |
| Sand/Mud/Dirt        |  Worn Surface            |                          |  Other                   |
|                      |                          |                          |  Rain                    |
|                      |                          |                          |  Severe Cross Wind       |
|                      |                          |                          |  Sleet/Hail              |
|                      |                          |                          |  Snow                    |
|                      |                          |                          |  Unknown                 |
Table 3: Different values for each of the attributes chosen for the predictive model

After removing these rows, performing one-hot encoding again and running the decision tree/random forest models again, the same level of accuracy of 78% was achieved. This indicated that the higher number of one-hot encoded columns is not impacting the prediction. Figure 12 below shows the confusion matrix after implementing the first approach. It can be seen from the plot why the accuracy stayed at 78%. The model predicted accurate values in some cases but at the same predicted inaccurate ones in other cases.

![
**Confusion plot after implementing the first approach**
](https://user-images.githubusercontent.com/112972950/205471191-a5a9a576-62ff-480f-add2-90e1b82da68a.png "approach1"){#fig: approach1}



#### 2- Taking into consideration another independent variable 
It was also decided to try considering one more column in the dataset and that could probably increase the accuracy. The reason behind this reasoning is if the model was offered more insight about the data, other than the surface road conditions, road defects, weather conditions and lighting conditions, this would probably help the model notice more relationships between the features and the dependent variable and thus increasing the accuracy. Therefore, and  based on the previous analysis of correlations between the columns, it was found that the presence or absence of road intersections was relatively highly correlated with crash severity, it was re-considered as one of the features for modelling. This attribute has 2 values: Yes or No. One-hot encoding was performed again and the decision tree model was applied again. However, the accuracy was also 78%. This showed that increasing the number of attributes is not the solution to low accuracy, in this case. Figure 13 below shows the confusion plot for the model after implementing approach 2, which is adding one the "IntersectionRelated" attribute. It can be seen that did not help make better predictions.

![
**Confusion plot after implementing the second approach**
](https://user-images.githubusercontent.com/112972950/205472284-edfd1e0c-af22-4e8b-8ebf-237f18b9e146.png "approach2"){#fig: approach3}


#### 3- Addressing the issue of imbalanced data
Because the problem was not the presence of several values for each column, dealing with the imbalanced dataset was another approach that was implemented by the team. This model is trying to predict the crash severity level which has three label options: "Property Damage", "Injury" and "Fatal". "Property Damage" constitutes roughly 80% of the data; 627706 rows out 809824 total rows (after cleaning the dataset), This imbalance is causing the model to predict "Property Damage" most of the time, decreasing the accuracy of the model. One way of addressing this is running the model with the same number of rows for each label.
"Fatal" had the lowest number of rows in the data which was 2694. Therefore, the same number of rows was randomly selected from each of the other two labels: Property Damage and Injury. This approach also yielded approcimately the same accuracy on the training data which is 80%. This can be noticed by observing the confusion plot for this model. (Figure 14)

![
**Confusion plot after implementing the third approach**
](https://user-images.githubusercontent.com/112972950/205474474-433aaf2a-1923-4f69-a214-9f7fe055c9a2.png "approach3"){#fig: approach3}

#### 4- Removing "perfect" conditions
Another way to look at the data imbalance of this dataset is by observing the features rather than the dependent variables. Part of the reason why the data is imbalanced is the fact that most of the time when the crashes happen, the coditions are "perfect", meaning the weather condition is clear, the road has no defcts, the lighting condition is daylight and the road surface condition is dry. Therefore, one approach taken by the team was to remove those conditions and try to run the decision tree model for the remaining cases. This increased the model accuracy, at best, to 85%. This can be seen is Figure 15 below. The fraction of accurate predictions increased, compared to previous models and approaches, but this is still low for a training set.

![
**Confusion plot after implementing the fourth approach**
](https://user-images.githubusercontent.com/112972950/205475750-4d45214e-7f69-4a4a-8d7f-ef4813fb0580.png "approach4"){#fig: approach4}

### c) Convolutional Neural Network (CNN)

According to Brunton and Kutz, practitioners would generally refer to Deep Convolutional Neural Networks when thinking of Neural Networks. Given its popularity, flexibility and effectiveness for classification problems, DCNN were used to model this problem.

The Deep Convolutional Neural Network was constructed using the Julia Machine Learning Library, Flux. This library allows for an easier construction of neural networks with predefined functions for different kinds of layers, activation functions, model training functions, and several other functions of interest when building up a neural network. 

For this model, as a first attempt, the entire dataset was used. An important consideration to account for when using the Flux library is that the neural network expects a very specific structure of the data: a 3 dimensional matrix of the input data that contains the observations divided into vectors, and one-hot encoded variables.

During the training phase, the model included convolutional layers, dense layers (to reduce the dimensionality of the ouput), the “ReLU” activation function (one of the more popular activation functions), pooling layers (for a more efficient computation and reduce overfitting), a dropout layer (to specifically target overfitting), and a “Softmax” function (a generalized version of the functions used for classification problems). These functions where all put together using a “Chain” function. In the testing phase, the Dropout function is disabled. 

The architecture of the Neural Network used is as follows:

```model =Chain(
    Conv((3,), 1=>2, pad=(1,)),
    MaxPool((2,)),
    Conv((3,), 2=>4, pad=(1,)),
    MaxPool((2,)),
    x -> reshape(x, :, size(x, 3)),
    Dropout(1),
    Dense(28,3),
    softmax
)```

The error metric used for training this Neural Network was the logistical cross entropy loss function. This function matches the “Softmax” function output, and works as a generalized version of cross entropy loss functions used for “more-than-two” classes classification problems.

The hyper parameters used to train the neural network consisted of a batch size of approximately one third of the length of the original dataset (300000 observations), which is consistent with the amount of observations collected in one year (as described previously, the dataset consist of a compilation of data from 3 different years). The number of steps or “epochs” for training was chosen empirically, with 100 steps consistently showing a plateau in the accuracy improvement. Finally, the learning rate was defined as 0.01. 

Similarly to the other predictive models used, the Convolutional Neural Network achieved an accuracy of 78.3%. The training process is illustrated in figure 10:

#### Convolutional Neural Network Training {.page_break_before}
![
**Using the complete dataset**
]( https://user-images.githubusercontent.com/112973190/202952710-056bd39e-6cab-4de5-a95f-d853c18e52a2.png "DCNN"){#fig: DCNN -full}

Given that the results did not show a significant improvement compared to the Decision Tree predictive models, a “worst-case scenario” of the data was generated and used to train this model. In an attempt to reduce the outstanding difference of “Crash Severity” conditions, this new dataset excluded the “Clear” weather condition, “No defects” road defect condition, and “Dry” road surface condition. Since the dataset reduce its size to approximately 20000 observations, the batch size was reduced respectively to train the model. 

With this “worst case scenario” dataset, the accuracy achieved reached 85.1%. The progress of training the Neural Network with this data are shown in figure 11:

#### Convolutional Neural Network Training {.page_break_before}
![
**Using a reduced dataset**
]( https://user-images.githubusercontent.com/112973190/202955768-b7fd5a48-6668-4564-82cc-d6a6da5ea847.png "DCNN2"){#fig: DCNN -reduced}

The results obtained using this predictive model may not be able to improve any further given the nature of the Convolutional Neural Networks and the dataset used to train the predictive models. It is relevant to note that: 

1. Convolutional Neural Networks are designed to pick up spatial patterns in "gridded" data, which is not one of the characteristics of the data analyzed in this project. Hence, this predictive model's effectiveness may be limited by the lack of spatial patterns in the data. 

2. Since the amount of “Property Damage” as one of the possible “Crash Severity” is overwhelmingly greater than the other 2 possible classes, the models tend to learn that the majority of combinations of the selected parameters would lead to a “Property Damage” prediction.


## Preliminary Conclusions

The main objective of this section was to go over the strategies for predicting the Crash Severity based on the Crash Datasets from the state of Illinois based on critical environmental/roadway-related characteristics. After the dataset preparation, which included the cleaning of the entries, as well as a thorough exploratory analysis of the data, the first steps were taken, starting from basic decision tree and random forest models, traditionally used for classification problems. For running any of these models, however, encoding the inputs was necessary, since these structures take numbers as inputs (not strings). The DT and RF models resulted in an accuracy that initially was interpreted as promising, but the limitations of the procedures taken were immediately realized, being the resulting models biased by the dominance of the “property damage” class on the dataset.

When the dataset was sampled to make the number of entries corresponding to every output even, the accuracy decreased, indicating that the dataset was not large enough to train the model with enough information. On the other hand, when the entries corresponding to the most common output were removed, the accuracy for predicting the two least likely outputs increased. Finally, when testing a Convolutional Neural Network, the accuracy of the predictions was once again around 78% considering a batch size of around a third part form the total number of entries, and using the dataset removing "perfect" conditions, accuracy reached 85%. The preliminary results evidence that further techniques may be applied to the original dataset to sample it a more convenient way to optimize the overall accuracy. The results of every model were tested for different parameters to verify the convergence and consistency of the predictions, which could be considered acceptable for a classification problem. However, for the future deliveries, further refinement may help to keep increasing the achieved accuracy, and a final consolidated model will therefore be assessed in terms of validation using datasets from years not considered for the training data.



